{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"Karachi is one of the most congested city in the world.\"\n",
    "str2 = \"Karachi is the financial capital of Pakistan.\"\n",
    "str3 = \"Islamabad is the capital of Pakistan.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', 'is', 'of', 'the', 'in'}\n",
      "['Karachi one most congested city world.', 'Karachi financial capital Pakistan.', 'Islamabad capital Pakistan.']\n"
     ]
    }
   ],
   "source": [
    "strlist = [str1,str2,str3]\n",
    "stopword = set(\"of the a is in\".split())\n",
    "print(stopword)\n",
    "s = []\n",
    "for str in strlist: \n",
    "    s_list = [word for word in str.split() if word not in stopword]\n",
    "    #print(s_list)\n",
    "    str_ = ' '.join(s_list)   \n",
    "    s.append(str_) \n",
    "print(s) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['capital', 'city', 'congested', 'financial', 'islamabad', 'karachi', 'most', 'one', 'pakistan', 'world']\n"
     ]
    }
   ],
   "source": [
    "cnt_vectorizer = CountVectorizer()\n",
    "cnt_vectorizer.fit(s)\n",
    "#print(cnt_vectorizer.vocabulary_)\n",
    "print(cnt_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 1 1 0 1]\n",
      " [1 0 0 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vec1 = cnt_vectorizer.transform(s).toarray()\n",
    "print(vec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiGram, TriGram and Limiting Maximum Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['capital', 'capital pakistan', 'city', 'city world', 'congested', 'congested city', 'financial', 'financial capital', 'islamabad', 'islamabad capital', 'karachi', 'karachi financial', 'karachi one', 'most', 'most congested', 'one', 'one most', 'pakistan', 'world']\n"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "bigram_vectorizer.fit(s)\n",
    "print(bigram_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 19)\n"
     ]
    }
   ],
   "source": [
    "vec2 = bigram_vectorizer.transform(s).toarray()\n",
    "print(vec2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.11396058 0.        ]\n",
      " [0.11396058 1.         0.50709255]\n",
      " [0.         0.50709255 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cs2 = cosine_similarity(vec2)\n",
    "print(cs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.20412415 0.        ]\n",
      " [0.20412415 1.         0.57735027]\n",
      " [0.         0.57735027 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cs1 = cosine_similarity(vec1)\n",
    "print(cs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['capital', 'capital pakistan', 'karachi', 'most', 'most congested', 'one', 'one most', 'pakistan']\n"
     ]
    }
   ],
   "source": [
    "#If not None, build a vocabulary that only consider the top max_features ordered by term frequency \n",
    "#across the corpus.\n",
    "bigram_vectorizer2 = CountVectorizer(ngram_range=(1, 2), max_features=8)\n",
    "bigram_vectorizer2.fit(s)\n",
    "print(bigram_vectorizer2.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.2236068 0.       ]\n",
      " [0.2236068 1.        0.8660254]\n",
      " [0.        0.8660254 1.       ]]\n"
     ]
    }
   ],
   "source": [
    "vec3 = bigram_vectorizer2.transform(s).toarray()\n",
    "cs3 = cosine_similarity(vec3)\n",
    "print(cs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subset of Amazon dataset used in this notebook can be downloaded from https://gist.github.com/kunalj101/ad1d9c58d338e20d09ff26bcc06c4235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#print(\"Current Working Directory \" , os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('DataSet/Amazon/corpus','r',encoding=\"utf8\") as f:\n",
    "    document = f.readlines()\n",
    "f.close()\n",
    "\n",
    "labels, texts = [], []\n",
    "for line in document:\n",
    "    content = line.split()\n",
    "    label = content[0]\n",
    "    labels.append(label[-1])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "print(len(labels), len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d747f7c69c60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtexts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "texts[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '2', '2', '2', '2', '2', '1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "#from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_vectorizer = CountVectorizer() #Try with different number of features and n-grams max_features=5000\n",
    "#cnt_vectorizer = CountVectorizer(ngram_range=(1, 3),max_features=3500)\n",
    "features = cnt_vectorizer.fit_transform(texts)\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 31627)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_nd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(features_nd, labels, train_size=0.75,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "labels = encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.838"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb_pred = nb.predict(X_test)\n",
    "accuracy_score(y_test,nb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7728"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=20)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "accuracy_score(y_test, rf_pred)nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb_pred = nb.predict(X_test)\n",
    "accuracy_score(y_test,nb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7212"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=5, learning_rate = 0.2)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_pred = gb.predict(X_test)\n",
    "accuracy_score(y_test, gb_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5000)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "features = tfidf_vectorizer.fit_transform(texts)\n",
    "features_nd = features.toarray()\n",
    "features_nd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(features_nd, labels, train_size=0.75,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.838"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb_pred = nb.predict(X_test)\n",
    "accuracy_score(y_test,nb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7768"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=20)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "accuracy_score(y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pakistan will address actionable evidence if shared by Delhi, PM Khan tells India after Pulwama attack\\n',\n",
       " 'Roadmap for $21bn Saudi investment finalised\\n',\n",
       " 'Pakistani nationals ordered to leave Indian city in 48 hours: TOI report\\n',\n",
       " 'PM Imran offers India probe into Kashmir suicide attack\\n',\n",
       " 'Riyadh to invest $21b over next five years\\n',\n",
       " 'Pakistanis visiting India given 48-hours to leave the country\\n',\n",
       " '“If India attacks, we will not think and retaliate back”: PM Imran\\n',\n",
       " 'Pakistan hails record Saudi investment as Crown Prince departs after first state visit\\n',\n",
       " 'Pakistan nationals told to leave Indian city within 48 hours\\n',\n",
       " 'Pakistan PM Imran Khan promises action if India shows Pulwama proof, warns against any rash move \\n',\n",
       " 'Saudi Arabia to invest 21 bln dollars in Pakistan in three phases\\n',\n",
       " \"Leave Within 48 Hours, Pak Nationals Told In Order In Rajasthan's Bikaner\"]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('headlines.txt','r',encoding=\"utf8\") as f:\n",
    "    document = f.readlines()\n",
    "f.close()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vectorizer = CountVectorizer() \n",
    "features = cnt_vectorizer.fit_transform(document)\n",
    "features_nd = features.toarray()\n",
    "features_nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 89)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_nd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 1 2 0 1 2 0 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "km = KMeans(n_clusters =3)\n",
    "km.fit(features_nd)\n",
    "new_labels = km.labels_\n",
    "print(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "cluster.fit_predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
